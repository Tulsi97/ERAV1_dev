{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tulsi97/ERAV1_dev/blob/develop/ERA_V1_S2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJptKBxALl-u",
        "outputId": "bd97b3ce-de95-4792-f768-8d2895188974"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"
          ]
        }
      ],
      "source": [
        "# Importing the necessary modules from the \n",
        "# PyTorch library and torchvision for working \n",
        "# with neural networks, datasets, and transformations.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if CUDA is available for GPU acceleration.\n",
        "use_cuda = torch.cuda.is_available()\n",
        "# Setting the device to either CUDA (GPU) if available or CPU if not.\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00Owi1LBNY8L",
        "outputId": "f3a9b2fb-2547-48de-e2d6-4a77aaf4259d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the batch size for data loading and training.\n",
        "batch_size = 128\n",
        "# Creating a data loader for the training dataset MNIST. \n",
        "# Downloading the dataset if not available, applying transformations\n",
        "# (converts to tensor and normalizes), and shuffling the data.\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "# Creating a data loader for the testing dataset MNIST. \n",
        "# Using the same transformations as the training dataset but setting the train flag to False.\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "EQZaZRGcNLtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some Notes on our naive model\n",
        "\n",
        "We are going to write a network based on what we have learnt so far. \n",
        "\n",
        "The size of the input image is 28x28x1. We are going to add as many layers as required to reach RF = 32 \"atleast\". "
      ],
      "metadata": {
        "id": "r3gEjf-xMb-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FirstDNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FirstDNN, self).__init__()\n",
        "        # Define the layers of the neural network\n",
        "        \n",
        "        # Convolutional layer 1: input channels=1, output channels=32, kernel size=3, padding=1\n",
        "        # r_in:1, n_in:28, j_in:1, s:1, r_out:3, n_out:28, j_out:1\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        \n",
        "        # Convolutional layer 2: input channels=32, output channels=64, kernel size=3, padding=1\n",
        "        # r_in:3 , n_in:28 , j_in:1 , s:1 , r_out:5 , n_out:28 , j_out:1\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        \n",
        "        # Max pooling layer 1: kernel size=2, stride=2\n",
        "        # r_in:5 , n_in:28 , j_in:1 , s:2 , r_out:6 , n_out:14 , j_out:2\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        \n",
        "        # Convolutional layer 3: input channels=64, output channels=128, kernel size=3, padding=1\n",
        "        # r_in:6 , n_in:14 , j_in:2 , s:1 , r_out:10 , n_out:14 , j_out:2\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        \n",
        "        # Convolutional layer 4: input channels=128, output channels=256, kernel size=3, padding=1\n",
        "        # r_in:10 , n_in:14 , j_in:2 , s:1 , r_out:14 , n_out:14 , j_out:2\n",
        "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        \n",
        "        # Max pooling layer 2: kernel size=2, stride=2\n",
        "        # r_in:14 , n_in:14 , j_in:2 , s:2 , r_out:16 , n_out:7 , j_out:4\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        \n",
        "        # Convolutional layer 5: input channels=256, output channels=512, kernel size=3\n",
        "        # r_in:16 , n_in:7 , j_in:4 , s:1 , r_out:24 , n_out:5 , j_out:4\n",
        "        self.conv5 = nn.Conv2d(256, 512, 3)\n",
        "        \n",
        "        # Convolutional layer 6: input channels=512, output channels=1024, kernel size=3\n",
        "        # r_in:24 , n_in:5 , j_in:4 , s:1 , r_out:32 , n_out:3 , j_out:4\n",
        "        self.conv6 = nn.Conv2d(512, 1024, 3)\n",
        "        \n",
        "        # Convolutional layer 7: input channels=1024, output channels=10, kernel size=3\n",
        "        # r_in:32 , n_in:3 , j_in:4 , s:1 , r_out:40 , n_out:1 , j_out:4\n",
        "        self.conv7 = nn.Conv2d(1024, 10, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Define the forward pass of the neural network\n",
        "        \n",
        "        # Apply convolutional layer 1, followed by ReLU activation function, and then max pooling\n",
        "        x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x)))))\n",
        "        \n",
        "        # Apply convolutional layer 3, followed by ReLU activation function, and then max pooling\n",
        "        x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
        "        \n",
        "        # Apply convolutional layer 5, followed by ReLU activation function\n",
        "        x = F.relu(self.conv6(F.relu(self.conv5(x))))\n",
        "        \n",
        "        # Apply convolutional layer 7, followed by ReLU activation function\n",
        "        x = F.relu(self.conv7(x))\n",
        "        \n",
        "        # Reshape the tensor to have a shape of (-1, 10)\n",
        "        x = x.view(-1, 10)\n",
        "        \n",
        "        # Apply the log softmax activation function\n",
        "        return F.log_softmax(x)"
      ],
      "metadata": {
        "id": "pZfoDmRfMzYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating object of class FirstDNN and adding it to the device specified above\n",
        "model = FirstDNN().to(device)"
      ],
      "metadata": {
        "id": "sxICO4TTNt2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# looking at the layer wise output dimensions and number of parameters for a given input size\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M__MtFIYNwXa",
        "outputId": "abca1ea8-7eb8-45ee-ea46-6a68e87161b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]             320\n",
            "            Conv2d-2           [-1, 64, 28, 28]          18,496\n",
            "         MaxPool2d-3           [-1, 64, 14, 14]               0\n",
            "            Conv2d-4          [-1, 128, 14, 14]          73,856\n",
            "            Conv2d-5          [-1, 256, 14, 14]         295,168\n",
            "         MaxPool2d-6            [-1, 256, 7, 7]               0\n",
            "            Conv2d-7            [-1, 512, 5, 5]       1,180,160\n",
            "            Conv2d-8           [-1, 1024, 3, 3]       4,719,616\n",
            "            Conv2d-9             [-1, 10, 1, 1]          92,170\n",
            "================================================================\n",
            "Total params: 6,379,786\n",
            "Trainable params: 6,379,786\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.51\n",
            "Params size (MB): 24.34\n",
            "Estimated Total Size (MB): 25.85\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-07eb6d900f57>:61: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    # Train the model\n",
        "    model.train()\n",
        "    \n",
        "    # Create a progress bar for visualization\n",
        "    pbar = tqdm(train_loader)\n",
        "    \n",
        "    # Iterate over the batches of data\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        # Move the data and target tensors to the device (GPU or CPU)\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        # Reset the gradients of the optimizer\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Perform forward pass to get the model's output\n",
        "        output = model(data)\n",
        "        \n",
        "        # Calculate the loss using negative log likelihood loss\n",
        "        loss = F.nll_loss(output, target)\n",
        "        \n",
        "        # Perform backward pass to compute the gradients of the parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update the parameters of the model using the gradients\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Update the progress bar description with current loss and batch index\n",
        "        pbar.set_description(desc=f'loss={loss.item()} batch_id={batch_idx}')\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    # Evaluate the model on the test set\n",
        "    model.eval()\n",
        "    \n",
        "    # Initialize variables for tracking test loss and accuracy\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    # Disable gradient calculation as we are only evaluating the model\n",
        "    with torch.no_grad():\n",
        "        # Iterate over the batches of data in the test set\n",
        "        for data, target in test_loader:\n",
        "            # Move the data and target tensors to the device (GPU or CPU)\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            \n",
        "            # Perform forward pass to get the model's output\n",
        "            output = model(data)\n",
        "            \n",
        "            # Calculate the loss using negative log likelihood loss, summing up the batch losses\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            \n",
        "            # Get the predicted labels by finding the index of the maximum log-probability\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            \n",
        "            # Count the number of correct predictions by comparing predicted labels with target labels\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    \n",
        "    # Calculate the average test loss\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    \n",
        "    # Print the test set results: average loss and accuracy\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))\n"
      ],
      "metadata": {
        "id": "g_vlC-bdNzo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "# Create an optimizer (Stochastic Gradient Descent) and pass the model parameters,\n",
        "# learning rate (lr) of 0.01, and momentum of 0.9\n",
        "\n",
        "for epoch in range(1, 2):\n",
        "    # Iterate over the specified range of epochs (in this case, 1 to 1)\n",
        "    \n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    # Call the train function to train the model on the training data.\n",
        "    # Pass the model, device, train_loader, optimizer, and epoch as arguments\n",
        "    \n",
        "    test(model, device, test_loader)\n",
        "    # Call the test function to evaluate the model on the test data.\n",
        "    # Pass the model, device, and test_loader as arguments"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0FYVWkGOFBS",
        "outputId": "62b6f01d-fb5b-4057-a903-51cf96579ee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/469 [00:00<?, ?it/s]<ipython-input-4-07eb6d900f57>:61: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n",
            "loss=0.06011991202831268 batch_id=468: 100%|██████████| 469/469 [00:34<00:00, 13.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0556, Accuracy: 9821/10000 (98%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-TrowljRPWEO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}