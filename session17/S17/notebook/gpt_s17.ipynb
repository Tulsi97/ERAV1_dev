{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOBsV0k0Dr7QCtoDbVrHsv9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"s9R-FTI0PT1r"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import sys\n","import os\n","sys.path.append(os.path.abspath(\"/content/drive/MyDrive/Colab Notebooks/ERAV1/S17/\"))\n","os.chdir(\"/content/drive/MyDrive/Colab Notebooks/ERAV1/S17/\")"],"metadata":{"id":"Xl26tCVxPd_V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls"],"metadata":{"id":"8a08PyWtPf3p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip --q install torchinfo\n","!pip --q install transformers"],"metadata":{"id":"hfaTCrMGPifz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","import torch.nn.functional as F\n","from collections import Counter\n","from os.path import exists\n","import torch.optim as optim\n","import torch.nn as nn\n","import numpy as np\n","import random\n","import torch\n","import math\n","import re\n"],"metadata":{"id":"iQR1pBpVPn7q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformer import TransformerGpt, TransformerBlockGpt, FeedForwardGpt, MultiHeadAttentionGpt, AttentionHeadGpt\n","\n","#gpt training\n","import torch\n","#from model import Transformer\n","from transformer import TransformerGpt\n","from transformers import AutoTokenizer  # pip install transformers\n","from GPT.utils import (\n","    BATCH_SIZE,\n","    BLOCK_SIZE,\n","    DEVICE,\n","    DROPOUT,\n","    LEARNING_RATE,\n","    NUM_EMBED,\n","    NUM_HEAD,\n","    NUM_LAYER,\n","    MAX_ITER,\n","    EVAL_INTER,\n","    encode,\n","    decode,\n","    get_batch,\n","    save_model_to_chekpoint,\n","    estimate_loss,\n",")\n","#DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","DEVICE"],"metadata":{"id":"zYl6cODPPoXN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load model from checkpoint\n","# m = load_model_from_checkpoint(Transformer,vocab_size=vocab_size)\n","\n","# example to decode sequence\n","# enc_sec = m.generate(idx=torch.zeros((1,1), dtype=torch.long),\n","# max_new_tokens=20)[0].tolist()\n","# print(decode(vocab=vocab, enc_sec=enc_sec))\n","\n","# raw data\n","path_do_data = \"GPT/data/english.txt\"\n","data_raw = open(path_do_data, encoding=\"utf-8\").read()\n","# we use pretrained BERT tokenizer for performance improvements\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","vocab_size = tokenizer.vocab_size\n","# data_raw = data_raw[4000000:] # short dataset\n","\n","# train/val split\n","data = encode(text_seq=data_raw, tokenizer=tokenizer)\n","n = int(0.9 * len(data))  # first 90% will be train, rest val\n","train_data = data[:n]\n","val_data = data[n:]\n","\n","# train a new model\n","model = TransformerGpt(\n","    vocab_size=vocab_size,\n","    num_embed=NUM_EMBED,\n","    block_size=BLOCK_SIZE,\n","    num_heads=NUM_HEAD,\n","    num_layers=NUM_LAYER,\n","    dropout=DROPOUT,\n",")\n","# load model to GPU if available\n","m = model.to(DEVICE)\n","# print the number of parameters in the model\n","print(\n","    \"Model with {:.2f}M parameters\".format(sum(p.numel() for p in m.parameters()) / 1e6)\n",")"],"metadata":{"id":"LqTu5vMpPxoj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# optimizer takes the model's parameters and the learning rate as input,\n","# and updates the parameters during the training process in order to\n","# minimize the loss function.\n","optimizer = torch.optim.AdamW(m.parameters(), lr=LEARNING_RATE)\n","MAX_ITER = 500\n","for step in range(MAX_ITER):\n","\n","    # every EVAL_INTER evaluate the loss on train and val sets\n","    if step % EVAL_INTER == 0 or step == MAX_ITER - 1:\n","        loss_train = estimate_loss(\n","            data=train_data, model=m, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE\n","        )\n","        loss_val = estimate_loss(\n","            data=val_data, model=m, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE\n","        )\n","        print(\"step {:10} | train loss {:6.4f} | val loss {:6.4f}\".format(step, loss_train, loss_val))\n","\n","    # sample a batch of data\n","    xb, yb = get_batch(data=train_data, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE)\n","    logits, loss = m.forward(xb, yb)\n","    # zero_grad() method sets the gradients of all parameters in the optimizer to zero\n","    optimizer.zero_grad(set_to_none=True)\n","    # backward() method on the loss variable calculates the gradients\n","    # of the loss with respect to the model's parameters.\n","    loss.backward()\n","    # step() method on the optimizer updates the model's parameters\n","    # using the calculated gradients, in order to minimize the loss.\n","    optimizer.step()\n","\n"],"metadata":{"id":"EpZhCMoAPz2y"},"execution_count":null,"outputs":[]}]}